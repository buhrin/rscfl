\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix,epsfig,authblk}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{listings}

\newcommand{\pname}{Resourceful}
\newcommand{\lnote}[1]{\textcolor{red}{[\textit{#1}]}} %notes
\newcommand*\aorder[1][\value{footnote}]{\footnotemark[#1]}
\renewcommand\Authsep{\hskip 1cm} \renewcommand\Authands{\hskip 1cm}

\colorlet{punct}{red!60!black}
\definecolor{background}{HTML}{EEEEEE}
\definecolor{delim}{RGB}{20,105,176}
\colorlet{numb}{magenta!60!black}

\lstdefinelanguage{json}{
    basicstyle=\normalfont\ttfamily,
    numberstyle=\scriptsize,
    stepnumber=1,
    numbersep=8pt,
    showstringspaces=false,
    breaklines=true,
    frame=lines,
    literate=
     *{0}{{{\color{numb}0}}}{1}
      {1}{{{\color{numb}1}}}{1}
      {2}{{{\color{numb}2}}}{1}
      {3}{{{\color{numb}3}}}{1}
      {4}{{{\color{numb}4}}}{1}
      {5}{{{\color{numb}5}}}{1}
      {6}{{{\color{numb}6}}}{1}
      {7}{{{\color{numb}7}}}{1}
      {8}{{{\color{numb}8}}}{1}
      {9}{{{\color{numb}9}}}{1}
      {:}{{{\color{punct}{:}}}}{1}
      {,}{{{\color{punct}{,}}}}{1}
      {\{}{{{\color{delim}{\{}}}}{1}
      {\}}{{{\color{delim}{\}}}}}{1}
      {[}{{{\color{delim}{[}}}}{1}
      {]}{{{\color{delim}{]}}}}{1},
}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    citecolor=black,
    filecolor=black,
    urlcolor=black,
}

\begin{document}
\lstdefinestyle{customc}{
  belowcaptionskip=1\baselineskip,
  frame=lines,
  breaklines=false,
  language=json,
  showstringspaces=false,
  basicstyle=\scriptsize\ttfamily,
}
%don't want date printed
\date{}

%make title bold and 14 pt font (Latex default is non-bold, 16 pt)
\title{\Large \bf \pname: Fine-grained Resource Accounting}

%for single author (just remove % characters)
\author{Lucian Carata\thanks{in alphabetical order}\aorder} \author{Oliver
Chick\aorder} \author{James Snee\aorder} \author{\authorcr{}Ripduman Sohan}
\author{Andrew Rice} \author{Andy Hopper} \affil{Computer Laboratory, University
of Cambridge, UK\\ \texttt{\{firstname.lastname\}@cl.cam.ac.uk}}

\maketitle

% Use the following at camera-ready time to suppress page numbers.
% Comment it out when you first submit the paper for review.
\thispagestyle{empty}


\subsection*{Abstract} We outline the design and prototype implementation of
\pname, a tool that allows fine-grained (system call level) resource accounting
for the Linux kernel. The system can account for both synchronous and
asynchronous CPU, memory and IO costs of given application calls, while
incurring a low overhead. \lnote{to be completed...}

\section{Introduction} 
%TODO(lc525): modify in accordance to Andy's comments

Any system experiencing high contention on particular resources will show more
variability in its behavior, becoming less predictable in terms of performance
and failure rates \cite{?}. This is why resource consumption metrics like CPU,
memory or IO are often used as a proxy for system behavior and health, in both
human-driven and automated processes (e.g. by an engineer evaluating increased
tail latency causes or a distributed scheduler making decisions about task
migration).

However, low resource usage is not desirable, either: there has been a constant
push for increasing server utilization and reducing running costs in modern
datacenters. Those are typically achieved through service aggregation, where
multiplexing several applications on a single physical host is done either
through hypervisor-based virtualization (VM's), lightweight virtualization
(containers), or by running multiple services on the same OS, depending on the
required level of isolation, SLAs, etc. %\lnote{In order to lower overheads, it
% seems likely that lighter-weight OS-based virtualization solutions such as
% containers will become more common.}
Here, the entities managing hardware resources (the hypervisor and/or the OS
kernel) are by design seen as black boxes by any process executing on top. This
makes it difficult to assess any side effects that a running process might have
upon other processes sharing the same resources. Those side effects could
manifest as performance degradation (for example, in cache trashing and IO
storms) or changes in the output of the system (fewer results returned, loss of
precision if running timeout-based processing algorithms).


Accurate resource accounting data is needed to measure, understand and mitigate
those implicit interractions.

%As more services will start sharing the same hardware resources, the importance
%of measuring and understanding both the causes and consequences of such
%variations in the properties of the system will increase. This justifies the
%need for gathering accurate resource accounting data \lnote{at the
%hypervisor/kernel level?}.

It is possible to make coarse-grained measurements in terms of
\textit{aggregate} resource consumption using existing profiling and monitoring
tools. Linux kernels provide mechanisms such as the \texttt{getrusage()} call
(small number of fixed statistics), \texttt{perf} (for reading performance
counters), and \texttt{ftrace}. Specialized tools such as \texttt{iotop} or
\texttt{netstat} use information exposed through the \texttt{$\backslash$proc}
virtual filesystem, while more general-purpose tools like SystemTap allow the
user to write scripts that can gather similar data. However, these methods fall
short in the following important dimensions:

(i) \textbf{Accounting granularity and aggregation:} most of the mechanisms
above can only obtain system-wide or per-process statistics. Therefore, it is
difficult to understand the contribution of a particular process
\textit{activity} towards the total resource consumption. For example, if one
wishes to diagnose occasional high latency responses from a server, then
per-process aggregated data cannot help. Instead, fine-grained information is
needed, such as per system call data aggregated only over the lifetime of the
request-response cycle. Existing tools that can track individual function calls,
such as \texttt{ftrace}, are often limited to debugging scenarios because of the overheads
they introduce.

(ii) \textbf{Accounting for resources consumed asynchronously:} not all the
effects of a given call occur during its execution. A very simple example here
is a process writing data to disk: while the application makes multiple calls to
\texttt{write(...)} on a particular file descriptor, there might be no immediate
IO activity on disk because the kernel uses a buffer cache. The actual disk IO
will occur asynchronously (dependent on the IO scheduler, the expiration of a
default flushing interval, an explicit call to \texttt{fsync}, etc). Simply
recording resource consumption metrics before and after a \texttt{write(...)}
call will not capture the full cost. The Linux kernel has multiple ways of
executing such asynchronous tasks (timers, tasklets, workqueues, software
interrupts), and understanding when they run because of particular application
actions is important in explaining shared resource usage. There are no existing
tools allowing this type of investigation.

(iii) \textbf{Online analysis and feedback:} with the exception of
\texttt{getrusage()}, most kernel-level resource accounting mechanisms available
are designed for debugging or offline analysis scenarios --- the monitored
processes have no control over what is being recorded and how, and the final
output is a log that needs to be processed in order to extract relevant data.
The applications themselves never have direct access to this data, and therefore
can not collaborate in avoiding resource contention when running concurrent
workloads (this is the usual model where applications have the illusion of
exclusive resource ownership).

In this paper we describe \pname, a framework that provides configurable
resource utilization measurements to applications interested in monitoring their
footprint in the context of overall resource consumption. \pname{ }addresses
the issues above through selective kernel probing, allowing data collection
down to the granularity of system calls. 

The main contributions of this paper are: 

\begin{itemize} 
\item Describing an architecture that allows applications to gather fine-grained
(system call level) resource consumption data broken down per kernel subsystem,
with low overhead.
\item Presenting a method for automatically identifying kernel subsystem
boundaries and the minimal number of required instrumentation probe points,
using static analysis.
\item Presenting a method for attributing resource consumption of asynchronous
tasks to the calls that triggered their execution.
\item Measuring overheads introduced by accounting for socket
accept/send/receive resource consumption in lighttpd, based on our prototype
implementation.
\end{itemize}

Our current implementation focuses on gathering resource usage data from the
Linux kernel.  However, the overall design is general enough to allow for
implementation in hypervisors and extension to other codebases. 
% set up reasonable expectations:
In this paper, we aim to show that it is possible to run a system tracking
resource consumption based on our design with reasonable overhead (\textless 10\%). A
full investigation on the accuracy of the recorded data remains as future work.


\section{\pname: System Design} 

\pname{ }gives applications full control over measuring resource consumption of
system calls, without imposing prohibitive overheads. The framework allowing
this has three main components: (i) \textit{measurement configuration:} this
analyzes the current kernel in order to identify minimal instrumentation probe
points and subsystem boundaries (the level at which aggregation takes place),
guided by a user-provided configuration; (ii) \textit{a kernel module} responsible for
inserting those probes into the kernel and efficiently activating them when
applications request resource consumption data and (iii) \textit{a user-space library}
exposing an API that applications can use to express interest in the resource
consumption of particular system calls and to read the results after the
required information was gathered on the kernel side.

Each resource accounting result provides detailed metrics grouped per kernel
subsystem (as configured by the first component of \pname). For example, the data recorded
for one system call would contain total CPU cycles, wall clock time and memory
allocated/deallocated, but the same metrics are recorded for each of the
subsystems touched during that call (total CPU cycles spent in the Network
subsystem, total cycles spent in VFS and subsystem-specific metrics such as
bytes sent/received, number of retransmissions, IO queue size, disk writes,
etc.). The application can select exactly which of those metrics are recorded
and can also perform custom aggregations across multiple system calls.

\subsection{Kernel Subsystem Identification}
The point of reporting resource consumption aggregated per subsystem is having a
more detailed view of what happens inside the kernel. Given a socket
\texttt{send(...)} operation, for example, the application can look at the
breakdown of latency: was most of the time spent in the network stack? was the
packet delayed by the scheduler moving the task on a different core? or perhaps
most of the time was spent reading data from a file descriptor on disk (in a 
\texttt{sendfile} call).
 
The Linux kernel has a modular structure of \emph{subsystems}, such as VFS,
logical filesystems, block devices. A complete list of these can be consulted in
the Linux Kernel Map.\footnote{\url{http://www.makelinux.net/kernel_map/}}

\pname{ }uses this logical structure for reporting the resource consumption on a
per-subsystem basis. 

%\lnote{(lc525): see par end} This differs from current approaches that typically fall
%into two categories: \textit{per-process} resource consumption, which lacks the
%resolution for determining the cause of bottlenecks and \textit{per-function}
%consumption, such as performed by ftrace. This has a high performance overhead
%(up to 3x in some situations), and produces lots of data requiring
%post-processing to extract usable information. 
%\lnote{(lc525): questionable whether we should have this here,
%we have mentioned before how we differ from others}

%Our approach allows us to give performance characteristics at a
%level whereby information can be used without post-processing, but with a high
%degree of granularity. Moreover, code paths changing subsystem is more rare than
%function calls, so we read counters less often, thereby achieving a lower
%overhead.

To identify subsystems, we perform inter-procedural static analysis of the
currently-running kernel. We start by finding all \texttt{call} instructions in
the kernel's binary, and determine to which function the callee and the caller
belong to. Each function is then categorized into one of the subsystems in the
Linux Kernel Map, predominantly based on its source file location. We mark the
function calls that are made from within one subsystem to another, and we
consider them to be part of the subsystem boundary.

\vspace{1em}
\lstset{style=customc, captionpos=b}
\begin{lstlisting}[caption={Sample configuration file defining a custom subsystem},label={lst:config}]
global {
  subsystem_whitelist: net_link_layer
}

subsystem net_link_layer {
  boundary:
    probe dev_queue_xmit {
      arg    : skb
      capture: {
        name: net_buf_enq,
        val : &skb->dev->qdisc
      }
    },
    probe qdisk_restart {
      arg    : dev
      capture: {
        name: net_buf_deq,
        val : &dev->qdisc
      }
    }
  metrics: cycles
  map_async: match(net_buf_deq, net_buf_enq)
}
\end{lstlisting}

Instrumentation probes need to be inserted around the place where such a boundary
function is called. Concretely, if the \texttt{SyS\_socket} function (from the
Network subsystem) calls \texttt{kmalloc} (from the Memory subsystem), we need
to add probes surrounding the call site of \texttt{kmalloc} within
\texttt{Sys\_socket}. Even if this means we could potentially be setting
numerous probes (a function such as kmalloc is called often), it is the
only way to avoid false positives: setting the probes inside \texttt{kmalloc}
would mean probing it even when it's not on a subsystem boundary (any call to
\texttt{kmalloc} from within the Memory subsystem). Whilst inserting more probes
has a higher startup cost, there is no runtime cost of unused probes and there
is no increase in code size from inserting unused probes.

Besides the subsystems identified automatically this way, we allow users to
influence the process through configuration files such as the one presented in
Listing~\ref{lst:config}. Here, the users can remove subsystems detected
automatically or add their own. The example shows how the network link layer
could be defined as a separate subsystem, generating probes for when packets are
enqueued/dequeued from device buffers.

So far, we have only applied our analysis to the Linux kernel. However, this
approach can be extended to any codebase where modules are organized using a
directory-based structure.

% Todo fn pointers
% Can be done on any kernel, without the need for source or recompilation.

\subsection{Kernel Tracing Infrastructure}

Having identified the minimum number of probe points required for measuring per-subsystem
resource consumption, a kernel module has the job of inserting those probes at runtime.
For the Linux kernel, this can be achieved using the existing kprobes mechanism.

Besides making sure that those probes run and snapshot resource consumption statistics
when required, the kernel module also needs to efficiently store this information
and make it accessible to user-space applications. We provide an overall schematic of
how this works in Figure~\ref{fig:design}.

\begin{figure}[ht!] 
	\centering 
	\includegraphics[width=1.1\columnwidth]{sys_design}
	\caption{Primary user-space/kernel-space interactions detailed. Resource
accounting data is requested by applications for specific system calls and is
then available for reading (zero-copy) from kernel buffer regions mmap-ed into
the current address space. } 
	\label{fig:design}
\end{figure}

On initialization, the kernel module creates two character devices: a data
device (\texttt{/dev/rscfl}) for resource accounting information and a control
device (\texttt{/dev/r\_ctl}) for communication between the user space and the module. When
applications linking with the \pname{ }library call \texttt{init()}, the two
devices are mmap-ed into the local address space. The \texttt{init()} function should be
called once for every thread of the application: on each mmap a new buffer is allocated
on the kernel side to hold resource accounting data for system calls made from that thread.

On hitting a probe, the module needs to determine the corresponding accounting
buffer for writing resource consumption data. An index that links given pids to
their corresponding buffers is maintained in per-cpu hash tables for this
purpose. \pname{ } intercepts calls to scheduler functions in order to maintain
those indexes up-to-date for each cpu.

\subsubsection{Measuring Asynchronous Effects}
Unlike existing approaches, \pname{} also reports the costs incurred
asynchronously after making a system call. As described in our example
use case, the absolute cost of performing a write that flushes buffers will be
higher than a write that is immediately buffered. We argue that without
reporting the amortized, asynchronous cost, performance monitoring APIs give an
incomplete representation of system resource consumption.

The solution for tracking the calls that have caused a given asynchronous task
hinges on the observation that a link between two operations can only exist if
the kernel code maintains some data structure shared by both. \textit{Buffers}
in which data is enqueued and later dequeued asynchronously (flushed) and
\textit{timers} started by a function for running a given task when reaching
zero are just two examples. Tracking the lifetime of those shared structures and
any actions performed on them is sufficient for determining the accounting
buffers to which resource consumption should be written to. 

When multiple system calls are amortized over the same asynchronous operation
(the example of multiple writes being buffered and then flushed to disk once),
the resources consumed by the flush will have to be divided amongst all the
initial system calls, following an attribution strategy. For buffered writes,
the simplest strategy is to divide the costs proportional to the size of each
write.

\pname{ }performs accounting for asynchronous kernel operations by observing that
Linux provides a number of abstractions for performing asynchronous work:
\emph{timers}, \emph{tasklets}, \emph{workqueues} and \emph{interrupts}. Each of
those is characterized by particular shared data structures, and the framework
will index their addresses in order to track the operation that triggered their
execution. 

Custom asynchronous accounting can also be set up:
Listing~\ref{lst:config} shows how network scheduler buffers (the qdisc buffer)
can be tracked across enqueue/dequeue operations: when the
\texttt{dev\_queue\_xmit} probe is hit (synchronously), \pname{ }will store the
address of the qdisc buffer. A following \texttt{qdisc\_restart} probe being hit
on an asynchronous path will match on the address of the qdisc buffer to assign
resource consumption to the initial accounting buffer.

% Async / Sync tracing (multiple layers, kworkers etc).
%Call granularity tracing.
%Trace-point idenfitication - CScope, call site vs function.
%Moving from ST to KProbes (more dynamic).

\subsection{User space API} 
A bitfield in the response lets the user know what data is available for consumption

Interface definition.

Discussion around automatically adding this (compiler).

\section{Prototype Implementation and Evaluation} Manual, cross-layer approach
while we implement the compiler based approach Implemented purely to show that
runtime overhead is small enough to not be a major issue Use coarse-grained
approach, using existing toold (kprobes, interrupt driven etc) Cross-layer
capture [Implentation details and results]

Describe use case (lighttpd).\newline End-to-end example of use of resourceful.
Show this is useful. `We show how this is useful.'

Overhead evaluation.\newline Space / time overhead. Mem. consumed. Time o/h of
various workloads.

\begin{figure}[ht!] 
	\centering 
	\includegraphics[width=1.1\columnwidth]{dist_and_fit}
	\caption{Change in latency distribution when enabling \pname{ }for lighttpd} 
	\label{fig:experiment1}
\end{figure}

Latency and throughput degradation.\newline For instrumented lighttpd. For
something else running at the same time.


\section{Related Work} Existing tracing / profiling systems. (Maybe a chart
outlining differences).

FTrace, SystemTap (raw) / DTrace, Perf.

Existing use of tracing for dependability.\newline Root-cause analysis.
Debugging. Performance monitoring - how do we compare to top?

\section{Conclusion} Open Challenges:\\ Accuracy and preciseness, Low
instrumentation overhead, Generalisability, Declarative Expressiveness

\section{Acknowledgments}

We thank everyone, and especially those who have funded our work.

{\footnotesize \bibliographystyle{acm} \bibliography{hotdep}}



\end{document}
