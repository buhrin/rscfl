\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix,epsfig,authblk}
\usepackage{xcolor}

\newcommand{\pname}{Resourceful}
\newcommand{\lnote}[1]{\textcolor{red}{[\textit{#1}]}} %notes
\newcommand*\aorder[1][\value{footnote}]{\footnotemark[#1]}
\renewcommand\Authsep{\hskip 1cm}
\renewcommand\Authands{\hskip 1cm}

\begin{document}

%don't want date printed
\date{}

%make title bold and 14 pt font (Latex default is non-bold, 16 pt)
\title{\Large \bf \pname: Fine-grained Resource Accounting}

%for single author (just remove % characters)
\author{Lucian Carata\thanks{in alphabetical order}\aorder}
\author{Oliver Chick\aorder}
\author{James Snee\aorder}
\author{\authorcr{}Ripduman Sohan}
\author{Andrew Rice}
\author{Andy Hopper}
\affil{Computer Laboratory, University of Cambridge, UK\\
       \texttt{\{firstname.lastname\}@cl.cam.ac.uk}}

\maketitle

% Use the following at camera-ready time to suppress page numbers.
% Comment it out when you first submit the paper for review.
\thispagestyle{empty}


\subsection*{Abstract}
We outline the design and prototype implementation of \pname, a tool that allows fine-grained (system call level) resource accounting for the Linux kernel. The system can account for both synchronous and asynchronous CPU, memory and IO costs of given application calls, while incurring a low overhead. \lnote{to be completed...}

\section{Introduction}
Any system experiencing high contention on particular resources will show more variability in its behavior, becoming less predictable in terms of performance and failure rates \cite{?}. This is why resource consumption metrics like CPU, memory or IO are often used as a proxy for system behavior and health, in both human-driven and automated processes (e.g. by an engineer evaluating increased tail latency causes or a distributed scheduler making decisions about task migration).

However, low resource usage is not desirable, either: there has been a constant push for increasing server utilization and reducing running costs in modern datacenters. Those are typically achieved through service aggregation, where multiplexing several applications on a single physical host is done either through hypervisor-based virtualization (VM's), lightweight virtualization (containers), or by running multiple services on the same OS, depending on the required level of isolation, SLAs, etc. 
%\lnote{In order to lower overheads, it seems likely that lighter-weight OS-based virtualization solutions such as containers will become more common.}
Here, the entities managing hardware resources (the hypervisor and/or the OS kernel) are by design seen as black boxes by any process executing on top. This makes it difficult to assess any side effects that a running process might have upon other processes sharing the same resources. Those side effects could manifest as performance degradation (for example, in cache trashing and IO storms) or changes in the output of the system (fewer results returned, loss of precision if running timeout-based processing algorithms).

As more services will start sharing the same hardware resources, the importance of measuring and understanding both the causes and consequences of such variations in the properties of the system will increase. This justifies the need for gathering accurate resource accounting data \lnote{at the hypervisor/kernel level?}.

It is of course possible to coarsely measure variation in terms of \textit{aggregate} resource consumption using existing profiling and monitoring tools. Linux kernels provide mechanisms such as the \texttt{getrusage()} call (small number of fixed statistics), \texttt{perf\_events} (for reading performance counters), and \texttt{ftrace}. Specialized tools such as \texttt{iotop} or \texttt{netstat} use information exposed through the \texttt{$\backslash$proc} virtual filesystem, and more general-purpose tools like SystemTap can be also used. However, we shall argue that those, by themselves, fall short in the following important dimensions: 

(i) Accounting granularity and aggregation: most of the mechanisms above can only obtain system-wide or per-process statistics. Because the data comes pre-aggregated, it becomes difficult to understand the contribution of a particular process activity (i.e. a system call or set of application function calls) towards the total resource consumption. For example, if a one tries to look at how resource utilization varies between low-latency and high-latency responses of a server (in order to do optimizations), it will not be able to do so based on per-process or even per-thread aggregated data. Instead, fine-grained (per system call, aggregated per request) information is needed. \lnote{Most systems avoid fine-grained accounting because of the overhead it introduces....}

(ii) Accounting for resources consumed asynchronously: not all the effects of a given call occur during its execution. However, there are no existing tools for linking asynchronous kernel tasks (together with the resources that they consume) to the application actions which triggered their execution. A very simple example here is a process writing data to disk: while the application makes multiple calls to \texttt{write(...)} on a particular file descriptor, there might be no immediate IO activity on disk because the kernel uses a buffer cache. The actual disk IO will occur asynchronously (dependent on the IO scheduler, the expiration of a default flushing interval, an explicit call to \texttt{fsync}, etc). Simply recording some resource consumption metrics before and after a \texttt{write(...)} call would not produce accurate results. The linux kernel has multiple ways of executing such asynchronous tasks (timers, tasklets, workqueues, software interrupts), and understanding when they run because of particular application actions is important in explaining shared resource usage.

(iii) Online analysis and feedback: with the exception of \texttt{getrusage()}, most kernel-level resource accounting mechanisms available are designed for debugging or offline analysis scenarios - the monitored processes have no control over what is being recorded and how, and the final output is a log that needs to be processed in order to extract relevant data. The applications themselves never have direct access to this data, and therefore can not collaborate in avoiding resource contention when running concurrent workloads.

As an answer to those issues, we outline the design and implementation of \pname, a framework that provides configurable resource utilization measurements to applications interested in monitoring their footprint in the context of overall resource consumption. Accurate data can be obtained with low overhead even at the granularity of individual system calls, taking into account resources consumed during the synchronous part of the call, as well as during any tasks triggered asynchronously. 

Even if our current prototype implementation focuses on the Linux kernel, the overall design is general enough to allow for implementation in hypervisors and extension to other codebases.

\section{Resourceful: System Design}
\pname{ } has three components: (i) a configuration component that analyzes the current kernel in order to identify subsystem boundaries (e.g. Network, VFS, Memory, etc.) and determines optimal probe points for accounting in each of them; (ii) a kernel module responsible for inserting those probes into the kernel and activating them at the request of applications and (iii) a user-space library offering an API through which applications pinpoint system calls for which resource accounting is desired, set up custom aggregation and read the results once they exposed by the kernel module. 
\subsection{Kernel Subsystem Identification}
\subsection{Kernel Tracing infrastructure}
Async / Sync tracing (multiple layers, kworkers etc).

Call granularity tracing.

Trace-point idenfitication - CScope, call site vs function.

Moving from ST to KProbes (more dynamic).

\subsection{User space API}
Interface definition.

Discussion around automatically adding this (compiler).	

\section{Prototype Implementation and Evaluation}
Manual, cross-layer approach while we implement the compiler based approach
Implemented purely to show that runtime overhead is small enough to not be a major issue
Use coarse-grained approach, using existing toold (kprobes, interrupt driven etc)
Cross-layer capture
[Implentation details and results]

Describe use case (lighttpd).\newline
End-to-end example of use of resourceful. Show this is useful. `We show how this
is useful.'

Overhead evaluation.\newline
Space / time overhead. Mem. consumed. Time o/h of various workloads.

Latency and throughput degradation.\newline
For instrumented lighttpd. For something else running at the same time.


\section{Related Work}
Existing tracing / profiling systems. (Maybe a chart outlining differences).

FTrace, SystemTap (raw) / DTrace, Perf.

Existing use of tracing for dependability.\newline
Root-cause analysis. Debugging. Performance monitoring - how do we compare to
top?

\section{Conclusion}
Open Challenges:\\
Accuracy and preciseness,
Low instrumentation overhead,
Generalisability,
Declarative Expressiveness

\section{Acknowledgments}

A polite author always includes acknowledgments.  Thank everyone,
especially those who funded the work.



{\footnotesize \bibliographystyle{acm}
\bibliography{hotdep}}



\end{document}
