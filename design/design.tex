\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\def\_{\textunderscore\-}

\title{Resourceful}
\author{Lucian Carata \and Oliver Chick \and James Snee}
\date{}
\begin{document}
\maketitle{}

\begin{abstract}
Some applications spend a significant amount of time in the Linux kernel \cite{kernelscale}, but understanding variations in execution time and other consumed resources (cpu, memory, disk, network) at the level of a system call remains challenging.
Existing methods to do resource accounting are either too coarse grained (per-task or system-wide tools like perf, iostats, latencytop) or too fine grained (ftrace with the function tracer) incurring significant overheads (3x) and requiring aggregation in user-space.
Furthermore, no attempts have been made to quantify delayed resource consumption generated by a particular system call. Such asynchronous operations are invisible from the perspective of the application, but add to the cost of running it and may affect the performance of other applications running concurrently.
\end{abstract}

\section{Resourceful---high level overview}
  As an answer to the above problems, we propose \emph{Resourceful}, a framework that reports syscall resource consumption aggregated per kernel functional unit \cite{kernelmap}. We report both costs that were incurred synchronously with the syscall and the ones incurred during asynchronous actions triggered by the syscall but executed after its completion (for example, the sync of a kernel buffer to disk, which may occur after the actual write syscall returned).

Resourceful exposes a user-space library that developers can use to express interest in the resource consumption made by specific system calls. Calls to this library can be made either explicitly by the developer or inserted as part of application instrumentation. The actual resource accounting takes place in the linux kernel (which we modify to support the required functionality).


We envision that the library, together with the changes made to the Linux kernel to support its features, provide the required primitives for implementing more complex use cases:

\subsection{Use cases}
\begin{enumerate}
\item Enforcing fine-grained quotas in running applications. We envision enabling quotas per application operation (in the example of a server application, per request quotas).
\item Tracking per-request resource consumption in a client/server setup and correlating that with variations in measured performance (end-to-end latency)
\item Explaining performance variations in virtualized environments: for example, understanding why doing I/O on Xen is sometimes slow.
\end{enumerate}


\subsection{Contributions}
\begin{itemize}
\item A low-overhead kernel-level mechanism for measuring per syscall resource consumption (cpu, memory, disk, network) in each functional module of the kernel (see the kernel map in [2] for a list).
\item Accounting for both synchronous and asynchronous resource consumption.
\end{itemize}


\section{System design}
 In this section we detail our design to expose primitives to user applications that let them better account for the resources spent by the kernel. We aim to provide these primitives with minimal impact on performance so that they can be feasibly used in a production system.

\subsection{User-space library}
\textbf{Developer’s perspective:\\}
The Resourceful library lets programmers express their interest in the measurement of resource consumption for the next syscall (the \texttt{acct\_next} function). After the next syscall returns, they can look into a \texttt{call\_cost} data structure and see the costs incurred synchronously by the syscall. When it is the case, the kernel will then fill in any asynchronous costs into the same data structure. To access those costs reliably, the user has to either call \texttt{wait\_async\_cost} passing the call\_cost structure or register a callback function (a function that will be called when the kernel detects all asynchronous effects of the syscall have been accounted for).
When calling \texttt{acct\_next} the developer can also specify which components should be accounted for (you can limit the amount of data being collected). Below we give an example of how we see this working in practice:
 \lstset{
  language=c,
  breaklines=true
  }
  \begin{lstlisting}
 void on_cost_done(call_cost* cost, void* ctx){
  //callback; read costs and take actions (aggregate, build histograms, etc)
 }

 int main(int argc, char** argv){
   init_resource_acct(); // called on each thread - does some memory allocation
			 // and calls an ioctl setting up resource accounting.
   int filter = SYS | PROC | NET_SOCK; // defines what resources we're
				       // interested in, default: ALL (include
				       // all resources)
   call_cost *cost_o, *cost_w;
   acct_next(&cost_o, filter);         // declares interest in measuring the
				       // resource consumption of the next syscall
   int fd = open("/../file_path", O_CREAT); // measured syscall (cost_o)

   char buf[BUF_SIZE];
   int sz = read(fd, buf, BUF_SIZE);        // syscall not measured

   acct_next(&cost_w, filter);
   // register callback for when the async part of the cost is fully computed
   cost_callback_async(cost_w, on_cost_done, 0);
   int res = write(fd, &buf, BUF_SIZE);     // measured syscall (cost_w)

   // if the write is asynchronous, cost_w will keep being updated by the
   // kernel for a while

   // do whatever you want with the call_cost data. you can read the sync
   // component as soon as the syscall is done. You should not touch the async
   // component until the kernel has set the async_done flag to true.
   // you can either wait with
   //    wait_async_cost(call_cost*);
   // or register a callback function using
   //    cost_callback_async(call_cost*, callback, ctx); (as above)

   close_resource_acct(); // de-allocate resource accounting structures
 }
   \end{lstlisting}
  \textbf{Library design:\\}

\begin{figure}
  \includegraphics[width=\textwidth]{figures/system-design.pdf}
  \caption{An overview of library internals}
  \label{fig:sysDesign}
\end{figure}


  We provide a library that lets programmers access a struct that describes the cost of a syscall. We require the program to specify that the next syscall should be accounted for, along with which components should be accounted for. After making the call, the user can access a struct that specifies the synchronous cost of making that syscall, per Linux subsystem. In particular, we do not perform function-level accounting, to reduce the overhead. Instead, we encourage developers to make use of existing tools, such as ftrace.

  \subsection{Userspace API}
  The resourceful API is exposed through a user-library.

  Before performing a syscall, a programmer can express that he wants the following syscall to be accounted for by calling the function \texttt{acct\_next}.
  \texttt{acct\_next} takes two arguments: a pointer to a \texttt{struct call\_cost} and a filter. The \texttt{call_cost} pointer is the address to which the kernel should write accounting information for the next syscall to be made.
  The filter is a bitmap representing which subsystems the user is interested in.

  The \texttt{struct sys\_acct} contains an enum map that shows which subsystems were accessed by the syscall followed by a section describing the costs for each subsystem touched. The programmer will read the touched subsystems, by reading the map, and then index into the relevant sections using offsets found from the map.


  \subsection{Library API}
  Our library has the following functions:
  \begin{itemize}
  \item \texttt{int call\_cost\_addr(struct call\_cost **, filter, n)} indicates that resourceful should track the next $n$ system calls to be made. The filter parameter indicates which subsystems it should be monitored through. The library is responsible for allocating memory for the accounting data to be stored in.\\
design note: because the programmer expresses interest above the glibc level (where the syscalls are typically made), we might miss resource accounting for some syscalls: whenever one glibc function calls multiple syscalls the user can only account for the resource usage of the first. If on the contrary, glibc buffers some input (writes) and only calls a syscall X once every n function calls, the user will end up sometimes measuring the cost of syscall $X$ and sometimes measuring the cost of whatever syscall comes next. We consider such glibc calls ``exotic'' and we delay dealing with them to the second implementation pass (it’s doable but requires more work).
  \item \texttt{int add\_to\_free\_stack(struct call\_cost *)} marks that the memory used by a \texttt{struct call\_cost} can be used to account for another syscall. We anticipate this being called after \texttt{memcpy}-ing the struct, or the data are no-longer required.
  \item \texttt{int add\_to\_free\_stack(struct call\_cost *[])} is the same as the above, but for multiple \texttt{struct call\_cost}s. This has a lower overhead, as only one mode change is required.
  \item \texttt{int stop\_async\_acct(struct call\_cost *)} prevents further accounting of the asynchronous costs in call\_cost.
  \end{itemize}

  This design puts responsibility for memory management on the library, rather than the kernel or the programmer. Naturally, programmers are not restricted to using our library.
  \begin{description}
  \item[On the first syscall of each thread] the library will allocate:
  \begin{enumerate}
  \item One 4K page, known as the \texttt{ctl\_page}.  This is mapped RW in userspace, and is used to indiate to the kernel: whether to account for a syscall; a pointer to where it should store accounting information; a bitmap of which subsystems we are interested in.
  \item Some huge pages. We anticipate 4MB of space. This memory will be used as a buffer to store all accounting information. We use huge pages to reduce our load on the TLB. This must be mapped RO in  userspace.
  \end{enumerate}
  Moreover, the library issues an \texttt{ioctl} to a char driver, to inform the kernel of the start address of this buffer. The ioctl checks the protection bits on the page and will fail if mapped RW.
  \item[On every syscall we are accounting for]:
  \begin{itemize}
  \item  the library finds some free memory. A sensible implementation would be using a stack that stores pointers to slots in the buffer. This will cause reuse of the same addresses, thereby reducing the rate of trashing caches.
  \item Writes the address that the accounting data should be stored in to the \texttt{ctl\_page}.
  \item Returns with a pointer to the address that a \texttt{struct call\_cost} will be stored.
  \end{itemize}
  \end{description}

  \subsection{Data structures}
  The top-level data structure is \texttt{struct call\_cost}. This is a nested struct that contains the accounting cost of a syscall, broken-down by subsystem. As most subsystems interact with CPU, memory, and at-most one other component (eg disk), we include a  bitmap  in struct call\_cost to indicate those components that are touched. We leverage this to allow us to store sparse data in the struct, thereby reducing our memory footprint.

  \texttt{struct call\_costs} are expected to be stored in huge pages. We leave the management of where to store the struct within the huge pages to userspace. These huge pages must be mapped read-only by the userspace.

  We also have a \texttt{struct ctl\_page}, which is a struct that exists for each thread and whose purpose it is to store the last address, and filters passed to \texttt{call\_cost\_addr}. Importantly we only have one value of this per thread, so each call to call\_cost\_addr will clobber the values written by the previous call. When a syscall is made  we copy, in the kernel, the values written here so that we still have a pointer to the the \texttt{struct call\_cost} after we have executed \texttt{sysexit}, since we need to attribute asynchronous costs.

  \section{Exposing kernel primitives}

  In this section we explore the modifications that we intend to make to Linux to allow fine-grained resource accounting. Our design requires us to write a char driver, and modify some integral kernel structs.

  Figure~\ref{fig:sysDesign} shows the high-level flow of resourceful programs. Each process that requires fine-grained resource accounting issues a single \texttt{ioctl} to a char driver. The driver will allocate a 4K page, known as the \texttt{ctl\_page}, that is mapped R/W to the current process, and has its address stored in the relevant \texttt{task\_struct}. We expect the process to write the \emph{virtual} address of the location that the kernel should store accounting information into \texttt{ctl\_page.call\_cost\_addr}.

Throughout the kernel we use the value of \texttt{call\_cost\_addr} as the identifier of where to store the accounting data.

  \subsection{Proposed modifications to kernel fundamentals}

We propose to modify syscalls  so they look to see if a new address has been written to the \texttt{ctl\_page}, and if so performs accounting.
 We shall modify \texttt{sysenter} so it starts by checking the \texttt{ctl\_page.call\_cost\_addr} and \texttt{ctl\_page.filter} to see if the syscall should be accounted for, and which metrics are desired. Should these be set, we:
  \begin{enumerate}
  \item Copy the \texttt{\&call\_cost\_addr} and \texttt{filter} to new fields added to \texttt{struct task\_struct}.
  \item Check the validity of the values.
  \item Convert the virtual address of \texttt{\&call\_cost\_addr} to the kernel-virtual address. We do this so the kernel can write to the address without having to walk page tables.
  \item Initialise \texttt{async\_ref\_cnt = 0}.
  \item Start relevant timers.
  \item Execute the system call.
  \end{enumerate}

  The syscall will then enter its synchronous phase. Each time the system call changes subsystem we propose to:
  \begin{enumerate}
  \item Read all timers that we expect to be running, as specified by \texttt{filter}.
  \item Add the costs of these to the relevant subsection of the \texttt{struct call\_cost}. We can get the address of the \texttt{struct call\_cost} by deferencing \texttt{current->call\_cost\_addr} since, during the synchronous part of the syscall, we can guarantee there will be no further syscalls made by that task.
  \end{enumerate}

  Before executing \texttt{sysexit}, we clobber \texttt{task\_struct.call\_cost\_addr} so as not to associated the next syscall with the same \texttt{call\_cost\_addr} should it not be accounted for.

  After executing \texttt{sysexit} the kernel may have some latent work to do as part of the asynchronous operations in the syscall. We intend to track the resources used by a syscall in both the synchronous, and asynchronous phases. As such we intend to modify the asynchronous mechanisms in Linux.  We note that Linux has a number of asynchronous mechanisms: \emph{kernel timers}, \emph{tasklets}, \emph{workqueues}, \emph{software interrupts}, and \emph{hardware interrupts}. To initialize each of these, the kernel calls a \texttt{\_\_init} function with a struct populated with relevant information. For instance to create a kernel timer, the \texttt{add\_timer} function\footnote{\texttt{kernel/timer.c}} is called with a \texttt{struct task\_struct}.

  As there are few functions on the interface to the kernel asynchronous mechanisms, each of which have a high number of call-sites, we intend \emph{not} to modify the structs on the interface.
  Rather we propose to encapsulate the \texttt{struct task\_struct} in \texttt{kernel/timer.c} to it the \texttt{call\_cost\_addr} in the current context.
  This mechanism allows us to append the address to which we shall store accounting information in asynchronous behaviour without breaking the interface.

  We believe that tracking the \texttt{call\_cost\_addr} associated with asynchronous actions in the kernel is necessary, as without doing so:
  \begin{enumerate}
  \item It is unclear where we ought to allocate memory to store the accounting costs. We would therefore have to map more kernel memory, thereby likely making less-aggressive memory re-use, thereby incurring more caches misses, and TLB misses.
  \item We would have to track every metric for every asynchronous action in the kernel.
  \item It is no longer clear when all asynchronous actions have terminated for a given syscall.
  \end{enumerate}

  \subsection{Low-overhead kernel mechanism}
  We envisage Resourceful being an always-on component that exposes fine-grained accounting to allow a richer class of application. We therefore have an emphasis on low-performance overhead. Our design has the following features to reduce overhead:
  \begin{description}
  \item[Low impact on TLB.] We use huge pages to reduce the load on the TLB.
  We anticipate ~4MB of continuous memory being required per process. Using 4K pages, this might use one quarter of the TLB. By using huge pages we reduce this to two entries (assuming 2MB pages).

  \item[Minimal locking.] We aim to reduce concurrency mechanisms to limit our performance overhead. In particular, we store accounting data on a per-thread basis.

  \item[Minimal mode changes.] \texttt{sysenter}/\texttt{sysexit}  add a non-trivial overhead, especially when we want to debug the performance of single syscalls. We therefore favour writing to shared-memory with atomic operations rather than making a syscall.

  \item[Zero-copy.] To improve performance we have avoided requiring the use of \texttt{memcpy}.

  \item[Shared memory strategy.] To reduce the number of mode switches, we make maximal use of shared memory. As a design choice we minimise pages being mapped RW to both userspace, and the kernel.

  The huge-pages that store \texttt{struct sys\_costs} are kernel pages that are mapped read-only into the user space application. Our design of sys\_costs includes pointers, and offsets that we follow in the kernel. To avoid trashing the kernel (and associated security issues) we prevent userspace from accessing these pages.

  \end{description}

  \subsection{Detecting subsystem boundaries}
  Resourceful accounts for resource consumption at a subsystem granularity.
  This requires us to define the functions that constitute a given subsystem and within these, the functions that act as an ingress point into it.
  Resourceful then uses these definitions to track which subsystem to credit resource consumption to.
  We employ a top-down approach when defining these systems by first observing the Linux Kernel source directory structure, which provides a list of 5 subsystems. Within each of these there are then further layers of abstraction that need to be accounted for. These further layers are manually defined with the aid of the Linux Kernel map.

  \begin{itemize}
      \item Storage (fs/)
      \begin{itemize}
	  \item Files and diectories
	  \item Virtual filesystem
	  \item Page cache, swap, network storage
	  \item Logical filesystem
	  \item Block devices
	  \item Disk controllers and drivers (drivers/)
      \end{itemize}
      \item Memory management (mm/)
      \begin{itemize}
	  \item Memory access
	  \item Virtual memory
	  \item Memory mapping, page cache, swap
	  \item Logical memory
	  \item Page allocator
	  \item Physical memory operations
      \end{itemize}
      \item Networking (net/)
      \begin{itemize}
	  \item Socket access
	  \item Protocol families
	  \item Networking storage
	  \item Protocols
	  \item Virtual network device
	  \item Nework device drivers (drivers/)
      \end{itemize}
      \item Processing (kernel/)
      \begin{itemize}
	  \item Processes
	  \item Threads
	  \item Synchronisation
	  \item Scheduler
	  \item Interrupt context
	  \item CPU specific (arch/)
      \end{itemize}
      \item System (kernel/)
      \begin{itemize}
	  \item System interfaces
	  \item Device model
	  \item System run
	  \item Generic HW access
	  \item Device access and bus drivers (drivers/)
      \end{itemize}
  \end{itemize}

  Note that some subsystems share functions for example the memory management subsystem implements part of the page cache along with the storage subsystem.
  Another observation is that due to the nature of abstraction, the lower the level the more machine specific, with many of the lowest levels either being implemented in device drivers (drivers/) or architecture specific code (arch/).


  In order to define the functions that constitute these subsystems we first use CTags to build a database of all functions along with the files they’re implemented in.
  Using this we are then able the tag each function with the top-level subsystem it belongs to (e.g. Networking || Processing).


  Resourceful accounts for resource consumption by checkpointing pre-defined resource counters at the entry and exit of each of these sub-systems.
  To enable this, the functions that act as entry points to these systems need to be identified, in order for them to be appropriately instrumented.
  These functions are identified by gathering function call traces of the target kernel using FTrace.
  The function calls in the resulting trace are then tagged with the top-level sub-system they belong to.
  The entry functions are then found by checking the boundaries between sub-systems, with the first function called in the sub-system being the entry.


  Further more specific sub-systems can be defined by annotating the original CTags list of symbols with the finer-grained system names. The same method as described above can be used to identify the sub-system entry functions.

  \subsection{Data structures required to store per-subsystem}
  Resourceful relies on a list of per sub-system entry functions in order to instrument the kernel to achieve per sub-system resource accounting. The following data format is used to store these definitions of sub-systems:

  \begin{lstlisting}
  <function_name> - <top-level system> : {sub-system , ...} - <filename>
  \end{lstlisting}

  e.g.
  \begin{lstlisting}
  __get_free_pages - memory : page_allocator - mm/page_alloc.c
  \end{lstlisting}

  Each of the functions named in the list is instrumented by Resourceful as an entry point into the sub-system it is labeled with.

  \section{Summary of work}

  We propose to write the following:
  \begin{description}
  \item[Write a char driver]
  \item[Provide a library that allows userspace programs to access accounting data]
  \item[Interpose the \texttt{sysenter} routine to initiate accounting]
  \item[Modify the major asynchronous structs]
  \item[Provide a method of mapping the kernel into subsystems]
  \end{description}


  \begin{thebibliography}{1}

  \bibitem{kernelscale} \url{http://people.csail.mit.edu/nickolai/papers/boyd-wickizer-scaling.pdf}
  \bibitem{kernelmap} \url{http://www.makelinux.net/kernel_map/}

  \end{thebibliography}


\end{document}